{
 "cells": [
  {
   "cell_type": "code",
   "id": "17ff16e9-8bfd-4fd2-bba2-ad3c313a2360",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-09-09T07:39:15.610175Z",
     "start_time": "2025-09-09T07:39:15.411394Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "hf = open(\"env.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = hf\n",
    "os.environ[\"HF_USERNAME\"] = \"smarcq\""
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'env.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mos\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m hf = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43menv.txt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mutf-8\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m.read()\n\u001B[32m      5\u001B[39m os.environ[\u001B[33m\"\u001B[39m\u001B[33mHF_TOKEN\u001B[39m\u001B[33m\"\u001B[39m] = hf\n\u001B[32m      6\u001B[39m os.environ[\u001B[33m\"\u001B[39m\u001B[33mHF_USERNAME\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33msmarcq\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages/IPython/core/interactiveshell.py:326\u001B[39m, in \u001B[36m_modified_open\u001B[39m\u001B[34m(file, *args, **kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m}:\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    321\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIPython won\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m by default \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    322\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    323\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33myou can use builtins\u001B[39m\u001B[33m'\u001B[39m\u001B[33m open.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    324\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'env.txt'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "10b790cd-5c91-4452-b705-c789862387f1",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-09T07:39:40.377541Z"
    }
   },
   "source": [
    "# install the libraries\n",
    "%pip install -U transformers \n",
    "%pip install -U accelerate \n",
    "%pip install -U datasets\n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install -U bitsandbytes "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\r\n",
      "Collecting filelock (from transformers)\r\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\r\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting numpy>=1.17 (from transformers)\r\n",
      "  Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from transformers) (6.0.2)\r\n",
      "Collecting regex!=2019.12.17 (from transformers)\r\n",
      "  Downloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\r\n",
      "Requirement already satisfied: requests in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from transformers) (2.32.5)\r\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\r\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Collecting safetensors>=0.4.3 (from transformers)\r\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\r\n",
      "Collecting tqdm>=4.27 (from transformers)\r\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\r\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\r\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\r\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)\r\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.6/11.6 MB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m  \u001B[33m0:00:02\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m561.5/561.5 kB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.2/3.2 MB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m4.8 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\r\n",
      "Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.9/16.9 MB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m  \u001B[33m0:00:03\u001B[0mm0:00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m799.0/799.0 kB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\r\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: tqdm, safetensors, regex, numpy, hf-xet, fsspec, filelock, huggingface-hub, tokenizers, transformers\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10/10\u001B[0m [transformers][0m [transformers]ub]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.9 huggingface-hub-0.34.4 numpy-2.3.2 regex-2025.9.1 safetensors-0.6.2 tokenizers-0.22.0 tqdm-4.67.1 transformers-4.56.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting accelerate\r\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from accelerate) (2.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from accelerate) (25.0)\r\n",
      "Requirement already satisfied: psutil in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from accelerate) (7.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from accelerate) (6.0.2)\r\n",
      "Collecting torch>=2.0.0 (from accelerate)\r\n",
      "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from accelerate) (0.34.4)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from accelerate) (0.6.2)\r\n",
      "Requirement already satisfied: filelock in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.9.0)\r\n",
      "Requirement already satisfied: requests in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.9)\r\n",
      "Collecting sympy>=1.13.3 (from torch>=2.0.0->accelerate)\r\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting networkx (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Requirement already satisfied: jinja2 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting triton==3.4.0 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\r\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from triton==3.4.0->torch>=2.0.0->accelerate) (78.1.1)\r\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.0.0->accelerate)\r\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/joachim/anaconda3/envs/crisalid-ai-duplicate-record-detection/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\r\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\r\n",
      "Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m156.5/888.1 MB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:01:19\u001B[0mm"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "32f36cee-cfe8-4213-b0ca-2566a2de1288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A100-SXM4-40GB\n",
      "25430362112\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1be08f4a-7666-452d-8555-774136165215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del model  # reset model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0e2f4f0-34df-4d0c-a165-d43a5dd2e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "991c7747-2a6b-4be2-b681-e84fd23aa5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "91412523-57d3-4e57-a052-a68b8d04e988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>reference_1</th>\n",
       "      <th>reference_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>287056</td>\n",
       "      <td>Distinct records</td>\n",
       "      <td>{'book': None, 'page': 'None-None', 'issue': {...</td>\n",
       "      <td>{'book': None, 'page': None, 'issue': {'date':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>287057</td>\n",
       "      <td>Distinct records</td>\n",
       "      <td>{'book': None, 'page': '94 - 102', 'issue': {'...</td>\n",
       "      <td>{'book': None, 'page': None, 'issue': None, 'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287059</td>\n",
       "      <td>Duplicates or equivalents</td>\n",
       "      <td>{'book': None, 'page': '653-658', 'issue': Non...</td>\n",
       "      <td>{'book': None, 'page': '653-658', 'issue': {'d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>287060</td>\n",
       "      <td>Duplicates or equivalents</td>\n",
       "      <td>{'book': None, 'page': None, 'issue': None, 'i...</td>\n",
       "      <td>{'book': None, 'page': 'None-None', 'issue': N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>287062</td>\n",
       "      <td>Distinct records</td>\n",
       "      <td>{'book': {'title': 'Virtual retrospect', 'isbn...</td>\n",
       "      <td>{'book': None, 'page': '11-17', 'issue': None,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                      label  \\\n",
       "0  287056           Distinct records   \n",
       "1  287057           Distinct records   \n",
       "2  287059  Duplicates or equivalents   \n",
       "3  287060  Duplicates or equivalents   \n",
       "4  287062           Distinct records   \n",
       "\n",
       "                                         reference_1  \\\n",
       "0  {'book': None, 'page': 'None-None', 'issue': {...   \n",
       "1  {'book': None, 'page': '94 - 102', 'issue': {'...   \n",
       "2  {'book': None, 'page': '653-658', 'issue': Non...   \n",
       "3  {'book': None, 'page': None, 'issue': None, 'i...   \n",
       "4  {'book': {'title': 'Virtual retrospect', 'isbn...   \n",
       "\n",
       "                                         reference_2  \n",
       "0  {'book': None, 'page': None, 'issue': {'date':...  \n",
       "1  {'book': None, 'page': None, 'issue': None, 'i...  \n",
       "2  {'book': None, 'page': '653-658', 'issue': {'d...  \n",
       "3  {'book': None, 'page': 'None-None', 'issue': N...  \n",
       "4  {'book': None, 'page': '11-17', 'issue': None,...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"20250708_filtered.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cfee418d-ad35-4054-8faa-b3470350cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the df into a 5500 record dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# shuffle and select 5500 lines\n",
    "dataset = dataset.shuffle(seed=85).select(range(5500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8eaad4b7-81dd-499d-b9ed-33b45fa5aeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ed80cc381342398dba7e43798b3c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# format prompts\n",
    "def format_chat_template(row):\n",
    "    user_content = f\"\"\"\n",
    "You are an expert in detecting duplicate research publications. Below are the metadata (in JSON) for two records collected from research data platforms. Your task is to analyze and classify them as one of:\n",
    "- \"Duplicates or equivalents\" — if they represent the same publication.\n",
    "- \"Distinct records\" — if they are two different records.\n",
    "- \"Insufficient information\" — if there is not enough evidence to decide.\n",
    "Do not explain your answer, only give one of the three labels listed above.\n",
    "\n",
    "Records:\n",
    "\n",
    "Record 1 metadata : {row['reference_1']}\n",
    "\n",
    "___________________________________________________\n",
    "\n",
    "Record 2 metadata : {row['reference_2']}\n",
    "\n",
    "Classification (Duplicates or equivalents / Distinct records / Insufficient information): \n",
    "\"\"\".strip()\n",
    "\n",
    "    if \"label\" in row and row[\"label\"] is not None:\n",
    "        assistant_content = row[\"label\"]\n",
    "    else:\n",
    "        assistant_content = \"\"\n",
    "\n",
    "    row[\"messages\"] = [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "    ]\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(format_chat_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "20810ff3-7908-409c-9235-9fb0f5bfa689",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an expert in detecting duplicate research publications. Below are the metadata (in JSON) for two records collected from research data platforms. Your task is to analyze and classify them as one of:\\n- \"Duplicates or equivalents\" — if they represent the same publication.\\n- \"Distinct records\" — if they are two different records.\\n- \"Insufficient information\" — if there is not enough evidence to decide.\\nDo not explain your answer, only give one of the three labels listed above.\\n\\nRecords:\\n\\nRecord 1 metadata : {\\'book\\': None, \\'page\\': \\'2370-2377\\', \\'issue\\': {\\'date\\': None, \\'number\\': [], \\'rights\\': None, \\'volume\\': \\'17\\', \\'journal\\': {\\'issn\\': [\\'1068-9265\\'], \\'eissn\\': [\\'1534-4681\\'], \\'titles\\': [\\'Annals of Surgical Oncology\\'], \\'publisher\\': \\'Springer Verlag\\'}}, \\'issued\\': \\'2010-01-01 00:00:00\\', \\'titles\\': [{\\'value\\': \\'Peritoneal Carcinomatosis from Gastric Cancer: A Multi-Institutional Study of 159 Patients Treated by Cytoreductive Surgery Combined with Perioperative Intraperitoneal Chemotherapy\\', \\'language\\': \\'en\\'}], \\'created\\': \\'2010-01-01 00:00:00\\', \\'subjects\\': [], \\'abstracts\\': [], \\'harvester\\': \\'HAL\\', \\'subtitles\\': [], \\'identifiers\\': [{\\'type\\': \\'hal\\', \\'value\\': \\'hal-02303675\\'}], \\'contributions\\': [{\\'rank\\': 0, \\'role\\': \\'https://id.loc.gov/vocabulary/relators/aut.html\\', \\'contributor\\': {\\'name\\': \\'O. Glehen\\', \\'source\\': \\'hal\\', \\'last_name\\': \\'Glehen\\', \\'name_variants\\': [\\'O. Glehen\\', \\'Olivier Glehen\\'], \\'source_identifier\\': \\'1359956\\'}}, {\\'rank\\': 1, \\'role\\': \\'https://id.loc.gov/vocabulary/relators/aut.html\\', \\'contributor\\': {\\'name\\': \\'F.N. Gilly\\', \\'source\\': \\'hal\\', \\'last_name\\': \\'Gilly\\', \\'name_variants\\': [], \\'source_identifier\\': \\'534685\\'}}, {\\'rank\\': 2, \\'role\\': \\'https://id.loc.gov/vocabulary/relators/aut.html\\', \\'contributor\\': {\\'name\\': \\'C. Arvieux\\', \\'source\\': \\'hal\\', \\'last_name\\': \\'Arvieux\\', \\'name_variants\\': [\\'Catherine Arvieux\\', \\'C. Arvieux\\'], \\'source_identifier\\': \\'1289649\\'}}, {\\'rank\\': 3, \\'role\\': \\'https://id.loc.gov/vocabulary/relators/aut.html\\', \\'contributor\\': {\\'name\\': \\'E. Cotte\\', \\'source\\': \\'hal\\', \\'last_name\\': \\'Cotte\\', \\'name_variants\\': [], \\'source_identifier\\': \\'434313\\'}}, {\\'rank\\': 4, \\'role\\': \\'https://id.loc.gov/vocabulary/relators/aut.html\\', \\'contributor\\': {\\'name\\': \\'Florent Boutitie\\', \\'source\\': \\'hal\\', \\'last_name\\': \\'Boutitie\\', \\'name_variants\\': [\\'F. Boutitie\\'], \\'source_identifier\\': \\'745182\\'}}, {\\'rank\\': 5, \\'role\\': \\'https://id.loc.gov/vocabulary/relators/aut.html\\', \\'contributor\\': {\\'name\\': \\'B. Mansvelt\\', \\'source\\': \\'hal\\', \\'last_name\\': \\'Mansvelt\\', \\'name_variants\\': [], \\'source_identifier\\': \\'558882\\'}}, {\\'rank\\': 6, \\'role\\': \\'https://id.loc.gov/vocabulary/relators/aut.html\\', \\'contributor\\': {\\'name\\': \\'J.M. Bereder\\', \\'source\\': \\'hal\\', \\'last_name\\': \\'Bereder\\', \\'name_variants\\': [], \\'source_identifier\\': \\'563098\\'}}, {\\'rank\\': 7, \\'role\\': \\'https://id.loc.gov/vocabulary/relators/aut.html\\', \\'contributor\\': {\\'name\\': \\'G. Lorimier\\', \\'source\\': \\'hal\\', \\'last_name\\': \\'Lorimier\\', \\'name_variants\\': [], \\'source_identifier\\': \\'558883\\'}}, {\\'rank\\': 8, \\'role\\': \\'https://id.loc.gov/vocabulary/relators/aut.html\\', \\'contributor\\': {\\'name\\': \\'F. Quenet\\', \\'source\\': \\'hal\\', \\'last_name\\': \\'Quenet\\', \\'name_variants\\': [], \\'source_identifier\\': \\'558879\\'}}, {\\'rank\\': 9, \\'role\\': \\'https://id.loc.gov/vocabulary/relators/aut.html\\', \\'contributor\\': {\\'name\\': \\'D. Elias\\', \\'source\\': \\'hal\\', \\'last_name\\': \\'Elias\\', \\'name_variants\\': [], \\'source_identifier\\': \\'380610\\'}}], \\'document_type\\': [{\\'uri\\': \\'http://purl.org/ontology/bibo/Article\\', \\'label\\': \\'Article\\'}], \\'manifestations\\': [{\\'page\\': \\'https://univ-lyon1.hal.science/hal-02303675v1\\'}], \\'source_identifier\\': \\'hal-02303675\\', \\'similarity_strategies\\': []}\\n\\n___________________________________________________\\n\\nRecord 2 metadata : {\\'book\\': None, \\'page\\': \\'20-20\\', \\'issue\\': {\\'date\\': None, \\'number\\': [\\'1\\'], \\'rights\\': None, \\'volume\\': \\'139\\', \\'journal\\': {\\'issn\\': [\\'0004-0010\\', \\'1538-3644\\'], \\'eissn\\': [], \\'titles\\': [\\'Archives of Surgery\\'], \\'publisher\\': \\'American Medical Association\\'}}, \\'issued\\': \\'2004-01-01 00:00:00\\', \\'titles\\': [{\\'value\\': \\'Cytoreductive Surgery and Intraperitoneal Chemohyperthermia for Peritoneal Carcinomatosis Arising From Gastric Cancer\\', \\'language\\': \\'en\\'}], \\'created\\': \\'2016-06-24 00:00:00\\', \\'subjects\\': [{\\'uri\\': \\'http://www.wikidata.org/entity/Q188874\\', \\'alt_labels\\': [{\\'value\\': \\'bowel cancer\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'colon and rectal cancer\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'rectocolonic cancer\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'cancer colo-rectal\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'cancer du colon\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'cancer colique\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'cancers du colon\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'colon cancer\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'rectal cancer\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'large intestine cancer\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'Colorectal cancer, familial\\', \\'language\\': \\'en\\'}], \\'pref_labels\\': [{\\'value\\': \\'cancer colorectal\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'colorectal cancer\\', \\'language\\': \\'en\\'}]}, {\\'uri\\': \\'http://www.wikidata.org/entity/Q172341\\', \\'alt_labels\\': [{\\'value\\': \\'Tumeur ovarienne\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'malignant tumour of ovary\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'primary ovarian cancer\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'malignant ovarian tumor\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'hereditary ovarian cancer\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'Familial ovarian malignant tumor\\', \\'language\\': \\'en\\'}], \\'pref_labels\\': [{\\'value\\': \"cancer de l\\'ovaire\", \\'language\\': \\'fr\\'}, {\\'value\\': \\'ovarian cancer\\', \\'language\\': \\'en\\'}]}, {\\'uri\\': \\'http://www.wikidata.org/entity/Q2113324\\', \\'alt_labels\\': [{\\'value\\': \\'Prospective Studies\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'prospective study\\', \\'language\\': \\'en\\'}], \\'pref_labels\\': [{\\'value\\': \\'prospective cohort study\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'étude longitudinale prospective\\', \\'language\\': \\'fr\\'}]}, {\\'uri\\': \\'http://www.wikidata.org/entity/Q11180\\', \\'alt_labels\\': [{\\'value\\': \\'Internal medicine\\', \\'language\\': \\'en\\'}], \\'pref_labels\\': [{\\'value\\': \\'internal medicine\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'médecine interne\\', \\'language\\': \\'fr\\'}]}, {\\'uri\\': \\'http://www.wikidata.org/entity/Q12078\\', \\'alt_labels\\': [{\\'value\\': \\'malignant tumor\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'primary cancer\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'tumeur maligne\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'malignant neoplasm\\', \\'language\\': \\'en\\'}], \\'pref_labels\\': [{\\'value\\': \\'cancer\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'cancer\\', \\'language\\': \\'fr\\'}]}, {\\'uri\\': \\'http://www.wikidata.org/entity/Q827390\\', \\'alt_labels\\': [{\\'value\\': \\'gastric resection\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'subtotal gastrectomy\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'total gastrectomy\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'stomach removal\\', \\'language\\': \\'en\\'}], \\'pref_labels\\': [{\\'value\\': \\'gastrectomie\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'gastrectomy\\', \\'language\\': \\'en\\'}]}, {\\'uri\\': \\'http://www.wikidata.org/entity/Q96375802\\', \\'alt_labels\\': [], \\'pref_labels\\': [{\\'value\\': \\'Cytoreductive surgery\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'Chirurgie cytoréductrice\\', \\'language\\': \\'fr\\'}]}, {\\'uri\\': \\'http://www.wikidata.org/entity/Q193889\\', \\'alt_labels\\': [{\\'value\\': \\'épanchement liquidien intraabdominal\\', \\'language\\': \\'fr\\'}], \\'pref_labels\\': [{\\'value\\': \\'ascite\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'ascites\\', \\'language\\': \\'en\\'}]}, {\\'uri\\': \\'http://www.wikidata.org/entity/Q837583\\', \\'alt_labels\\': [{\\'value\\': \\'carcinomatosis\\', \\'language\\': \\'en\\'}], \\'pref_labels\\': [{\\'value\\': \\'carcinosis\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'carcinose\\', \\'language\\': \\'fr\\'}]}, {\\'uri\\': \\'http://www.wikidata.org/entity/Q40821\\', \\'alt_labels\\': [{\\'value\\': \\'Chirurgie fonctionnelle\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'Chirurgie générale\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'surgical procedure\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'surgical specialty\\', \\'language\\': \\'en\\'}], \\'pref_labels\\': [{\\'value\\': \\'chirurgie\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'surgery\\', \\'language\\': \\'en\\'}]}, {\\'uri\\': \\'http://www.wikidata.org/entity/Q2071182\\', \\'alt_labels\\': [{\\'value\\': \\'Carcinose Péritonéale\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'Carcinose peritoneale\\', \\'language\\': \\'fr\\'}, {\\'value\\': \\'Peritoneal metastases\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'Secondary tumors of the peritoneal\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'Abdominal Carcinomatosis\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'Carcinomatosis peritonei\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'Carcinomatosis of the Peritoneum\\', \\'language\\': \\'en\\'}], \\'pref_labels\\': [{\\'value\\': \\'peritoneal carcinomatosis\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'carcinose péritonéale\\', \\'language\\': \\'fr\\'}]}, {\\'uri\\': \\'http://www.wikidata.org/entity/Q11190\\', \\'alt_labels\\': [{\\'value\\': \\'medical\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'medicalism\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'clinical medicine\\', \\'language\\': \\'en\\'}], \\'pref_labels\\': [{\\'value\\': \\'medicine\\', \\'language\\': \\'en\\'}, {\\'value\\': \\'médecine\\', \\'language\\': \\'fr\\'}]}], \\'abstracts\\': [{\\'value\\': \\'Hypothesis:The most common cause of palliative resection and recurrence in gastric cancer is peritoneal seeding.This study evaluates the efficacy intraperitoneal chemohyperthermia after cytoreductive surgery patients with carcinomatosis arising from cancer.Design: Prospective clinical trial.Setting: Surgical department at a university academic hospital.Patients: Forty-nine consecutive treated between January 1\\', \\'language\\': \\'en\\'}], \\'harvester\\': \\'OpenAlex\\', \\'subtitles\\': [], \\'identifiers\\': [{\\'type\\': \\'pmid\\', \\'value\\': \\'https://pubmed.ncbi.nlm.nih.gov/14718269\\'}, {\\'type\\': \\'doi\\', \\'value\\': \\'10.1001/archsurg.139.1.20\\'}, {\\'type\\': \\'open_alex\\', \\'value\\': \\'https://openalex.org/W2081516783\\'}], \\'contributions\\': [{\\'rank\\': 1, \\'role\\': \\'https://id.loc.gov/vocabulary/relators/aut.html\\', \\'contributor\\': {\\'name\\': \\'Olivier Gléhen\\', \\'source\\': \\'open_alex\\', \\'last_name\\': None, \\'name_variants\\': [], \\'source_identifier\\': \\'https://openalex.org/A5031036666\\'}}], \\'document_type\\': [{\\'uri\\': \\'http://purl.org/ontology/bibo/Article\\', \\'label\\': \\'Article\\'}], \\'manifestations\\': [{\\'page\\': \\'https://pubmed.ncbi.nlm.nih.gov/14718269\\'}, {\\'page\\': \\'https://doi.org/10.1001/archsurg.139.1.20\\'}], \\'source_identifier\\': \\'https://openalex.org/W2081516783\\', \\'similarity_strategies\\': []}\\n\\nClassification (Duplicates or equivalents / Distinct records / Insufficient information):', 'role': 'user'}, {'content': 'Distinct records', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "# Split to train/test (test_size=0.2) --\n",
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# split test to eval/test -> 80/10/10\n",
    "eval_test_split = dataset_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "final_splits = {\n",
    "    \"train\": dataset_split[\"train\"],\n",
    "    \"eval\": eval_test_split[\"train\"],\n",
    "    \"test\": eval_test_split[\"test\"]\n",
    "}\n",
    "\n",
    "# print firtst prompt\n",
    "print(final_splits[\"train\"][\"messages\"][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "754b37b8-84c3-423e-82de-bbb7ac2a9e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0000f8d2ed843c1be3178aca2293db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fac8efeb6394f83a70a6f679e5e96be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd97ee12130e46038543c1f5f51a5d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9362b87e153448879507acb2db9000f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_test_split.save_to_disk(\"dataset_eval_test\")\n",
    "dataset_split.save_to_disk(\"dataset_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e57b9efe-5595-4be7-ac17-0118436b5365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Label distribution BEFORE filtering\n",
      "\n",
      "Label distribution in train set:\n",
      "  Distinct records: 3299 (74.98%)\n",
      "  Duplicates or equivalents: 639 (14.52%)\n",
      "  Insufficient information: 462 (10.50%)\n",
      "\n",
      "Label distribution in eval set:\n",
      "  Distinct records: 396 (72.00%)\n",
      "  Insufficient information: 66 (12.00%)\n",
      "  Duplicates or equivalents: 88 (16.00%)\n",
      "\n",
      "Label distribution in test set:\n",
      "  Distinct records: 412 (74.91%)\n",
      "  Insufficient information: 58 (10.55%)\n",
      "  Duplicates or equivalents: 80 (14.55%)\n"
     ]
    }
   ],
   "source": [
    "# print label distribution\n",
    "from collections import Counter\n",
    "\n",
    "def display_label_distribution(splits_dict, title):\n",
    "    print(f\"\\n🔎 {title}\")\n",
    "    for split_name, split_data in splits_dict.items():\n",
    "        label_counts = Counter(split_data[\"label\"])\n",
    "        print(f\"\\nLabel distribution in {split_name} set:\")\n",
    "        total = sum(label_counts.values())\n",
    "        for label, count in label_counts.items():\n",
    "            pct = (count / total) * 100 if total > 0 else 0\n",
    "            print(f\"  {label}: {count} ({pct:.2f}%)\")\n",
    "\n",
    "display_label_distribution(final_splits, \"Label distribution\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6722d1de-f6ad-4926-a731-d946e63931d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a71290cb-ea63-4518-aa55-7cceeee43360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50323de2715143ba97af9e49b40e2591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the model\n",
    "base_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "output_dir=\"./llama-3-crisalid-drd\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True, # quantization 8 bits\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dbda2707-64d9-4491-bc48-7834b4c73219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "86af7e8e-bb60-4249-a35c-d86bd10e0bef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Stats for split: train\n",
      "Min tokens: 611\n",
      "Max tokens: 18852\n",
      "Mean tokens: 3069.54\n",
      "\n",
      "📊 Stats for split: eval\n",
      "Min tokens: 729\n",
      "Max tokens: 17528\n",
      "Mean tokens: 3292.44\n",
      "\n",
      "📊 Stats for split: test\n",
      "Min tokens: 647\n",
      "Max tokens: 17389\n",
      "Mean tokens: 3092.37\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def count_tokens(messages):\n",
    "    full_prompt = \"\"\n",
    "    for message in messages:\n",
    "        full_prompt += message[\"role\"] + \": \" + message[\"content\"] + \"\\n\"\n",
    "    tokens = tokenizer.tokenize(full_prompt)\n",
    "    return len(tokens)\n",
    "\n",
    "for split_name, split_data in final_splits.items():\n",
    "    token_lengths = [count_tokens(example[\"messages\"]) for example in split_data]\n",
    "    print(f\"\\n📊 Stats for split: {split_name}\")\n",
    "    print(f\"Min tokens: {np.min(token_lengths)}\")\n",
    "    print(f\"Max tokens: {np.max(token_lengths)}\")\n",
    "    print(f\"Mean tokens: {np.mean(token_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6bdbf656-1416-4a85-8cd0-05b6c465a8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705174730ca4435caf6c6ab757de593f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/4400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17bc651477e44a8b2cc0f2cdb2cbba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/4400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728e74608abb4777bff7844609792748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b5d573b3be442597b517626f8ee049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fune tuning config\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=2,\n",
    "    eval_steps=0.5,\n",
    "    logging_steps=10,\n",
    "    warmup_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=3e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    group_by_length=True,\n",
    "    label_names=[\"Duplicates or equivalents\", \"Distinct records\", \"Insufficient information\"],\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=final_splits[\"train\"],\n",
    "    eval_dataset=final_splits[\"eval\"],\n",
    "    peft_config=peft_config,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779c612-d320-45f7-b3af-f8d9adc25056",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/550 31:39 < 25:24, 0.16 it/s, Epoch 1.11/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.901200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.558700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.504900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.446800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.453200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.461700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.402600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.354900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.358200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.339400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.259600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.223900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bf6b6-5799-4ff3-983b-c1c9cbf869ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0433a-7492-4770-9869-6c5813a3022f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7ee03-352d-4ab3-9bb2-48efc87d6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec389cc-9b5a-4d8c-b5d0-4d6275b06bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117d014-f1e0-4f14-aac7-408b50bf40bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f783bf-bdbe-44eb-8eea-1f847fb5c12f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to do : regarder quand se créent les backslash"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
